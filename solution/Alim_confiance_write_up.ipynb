{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cstrachan342/Alim-Confiance/blob/main/solution/Alim_confiance_write_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Creation**\n",
        "\n",
        "In this notebook I will detail how I created my Dataset by sourcing and merging auxiliary datasets. By doing so I add contextual information providing additional dimensions to the data, which will enable models to gain an understanding of complex relationships to improve overal predictive performance.\n"
      ],
      "metadata": {
        "id": "xwvvvtvVrVvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cstrachan342/Alim-Confiance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htYyh9QKyApr",
        "outputId": "f27aed86-38b0-4fa7-b338-fb4afff7f098"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Alim-Confiance'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 43 (delta 8), reused 34 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (43/43), 8.72 MiB | 20.15 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "irLi99igxQWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5febd39-3a3e-4f7c-d433-4d9c35b06474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q category_encoders\n",
        "!pip install -q haversine\n",
        "!pip install -q unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iimNWHFDxOb3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "from datetime import datetime\n",
        "from unidecode import unidecode\n",
        "import difflib\n",
        "import haversine as hs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import category_encoders as ce\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The challenge  was not a time series problem as it initially looks like. From my feature engineering below you will see I have treated the dataset **mostly** as a time series problem with aspects that are not time series. This was the most successful way I found to optimise the given metric of accuracy. If I was using the Alim'Confidance dataset for the stereotypical use I would treat it solely as a time series issue and not engineer some of the features I did.  "
      ],
      "metadata": {
        "id": "cnzgW1D5Gdtm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xdusK0DNwlQO"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/Alim-Confiance/data/alim_confiance_dataset/alim_confiance_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "dZSsVGHhHnCG",
        "outputId": "506e92c6-cfb3-408f-a837-4009f79151f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       APP_Libelle_etablissement           SIRET            Adresse_2_UA  \\\n",
              "0             SAVEURS ASIATIQUES  50846842800034  RUE DES FRERES LUMIERE   \n",
              "1            EPICERIE ROND POINT  90472398800013     48 AV DU ROND POINT   \n",
              "2           AUBERGE DES OLIVIERS  48792443300013    ROUTE DE LA CANONICA   \n",
              "3          LES HUITRES DES FLOTS  80481822700022  RUE DU PONT DES BERNES   \n",
              "4      COLONNA DOMINIQUE-ANTOINE  53444598600019              Partinello   \n",
              "...                          ...             ...                     ...   \n",
              "22666  AUX DELICES DE CHARENTONG  88764629700010         28 RUE DE PARIS   \n",
              "22667            PICARD SURGELES  78493968801468      15 BOULEVARD BRUNE   \n",
              "22668         CFA DE HAUTE SAONE  13002607300135        5 RUE DU TALEROT   \n",
              "22669                LA FABRIQUE  49360317900042             LA FABRIQUE   \n",
              "22670            SOC REGAL MAREE  32732547800037  12_ Rue Albert LAVOCAT   \n",
              "\n",
              "      Code_postal            Libelle_commune Numero_inspection  \\\n",
              "0           33130                     Bègles       23-110093-1   \n",
              "1           93250                Villemomble       23-056603-1   \n",
              "2           20290                   Lucciana       23-018114-1   \n",
              "3           50550      Saint-Vaast-la-Hougue       23-011747-1   \n",
              "4           20147                 Partinello       23-097885-1   \n",
              "...           ...                        ...               ...   \n",
              "22666       94220          Charenton-le-Pont       23-009992-1   \n",
              "22667       75014  Paris 14e  Arrondissement       23-051981-1   \n",
              "22668       70000                     Vesoul       23-048979-1   \n",
              "22669       08000       Charleville-Mézières       23-069730-1   \n",
              "22670       62200           Boulogne-sur-Mer       23-028895-1   \n",
              "\n",
              "                 Date_inspection  \\\n",
              "0      2024-02-05T01:00:00+01:00   \n",
              "1      2023-07-17T02:00:00+02:00   \n",
              "2      2023-03-08T01:00:00+01:00   \n",
              "3      2023-02-15T01:00:00+01:00   \n",
              "4      2023-11-21T01:00:00+01:00   \n",
              "...                          ...   \n",
              "22666  2023-02-14T01:00:00+01:00   \n",
              "22667  2023-05-26T02:00:00+02:00   \n",
              "22668  2023-06-21T02:00:00+02:00   \n",
              "22669  2023-08-16T02:00:00+02:00   \n",
              "22670  2023-03-29T02:00:00+02:00   \n",
              "\n",
              "                    APP_Libelle_activite_etablissement Synthese_eval_sanit  \\\n",
              "0                                           Restaurant        Satisfaisant   \n",
              "1                  Libre service|Alimentation générale        Satisfaisant   \n",
              "2                                           Restaurant   Très satisfaisant   \n",
              "3               Purification/Expédition de coquillages        Satisfaisant   \n",
              "4                                   Producteur fermier   Très satisfaisant   \n",
              "...                                                ...                 ...   \n",
              "22666                                       Restaurant        Satisfaisant   \n",
              "22667                                    Libre service   Très satisfaisant   \n",
              "22668       Découpe de viandes de volailles/lagomorphe   Très satisfaisant   \n",
              "22669                                       Restaurant   Très satisfaisant   \n",
              "22670  Mareyage et préparation de produits de la pêche        Satisfaisant   \n",
              "\n",
              "       Agrement               geores                               filtre  \\\n",
              "0           NaN  44.797031_-0.535231                           Restaurant   \n",
              "1           NaN   48.884745_2.499984  Libre service|Alimentation générale   \n",
              "2           NaN    42.541715_9.46286                           Restaurant   \n",
              "3      50562016   49.592002_-1.28678                                  NaN   \n",
              "4           NaN    42.306077_8.67833                   Producteur fermier   \n",
              "...         ...                  ...                                  ...   \n",
              "22666       NaN   48.818757_2.417811                           Restaurant   \n",
              "22667       NaN   48.825397_2.314932                        Libre service   \n",
              "22668  70550005   47.633819_6.155241                                  NaN   \n",
              "22669       NaN   49.772441_4.720325                           Restaurant   \n",
              "22670  62160172   50.722087_1.593044                                  NaN   \n",
              "\n",
              "                       ods_type_activite  \n",
              "0                                 Autres  \n",
              "1                                 Autres  \n",
              "2                                 Autres  \n",
              "3      Produits de la mer et d'eau douce  \n",
              "4                                 Autres  \n",
              "...                                  ...  \n",
              "22666                             Autres  \n",
              "22667                             Autres  \n",
              "22668         Viandes et produits carnés  \n",
              "22669                             Autres  \n",
              "22670  Produits de la mer et d'eau douce  \n",
              "\n",
              "[22671 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-949c828f-7b56-4f31-9254-95e8067c358d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>APP_Libelle_etablissement</th>\n",
              "      <th>SIRET</th>\n",
              "      <th>Adresse_2_UA</th>\n",
              "      <th>Code_postal</th>\n",
              "      <th>Libelle_commune</th>\n",
              "      <th>Numero_inspection</th>\n",
              "      <th>Date_inspection</th>\n",
              "      <th>APP_Libelle_activite_etablissement</th>\n",
              "      <th>Synthese_eval_sanit</th>\n",
              "      <th>Agrement</th>\n",
              "      <th>geores</th>\n",
              "      <th>filtre</th>\n",
              "      <th>ods_type_activite</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAVEURS ASIATIQUES</td>\n",
              "      <td>50846842800034</td>\n",
              "      <td>RUE DES FRERES LUMIERE</td>\n",
              "      <td>33130</td>\n",
              "      <td>Bègles</td>\n",
              "      <td>23-110093-1</td>\n",
              "      <td>2024-02-05T01:00:00+01:00</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>Satisfaisant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>44.797031_-0.535231</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>Autres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EPICERIE ROND POINT</td>\n",
              "      <td>90472398800013</td>\n",
              "      <td>48 AV DU ROND POINT</td>\n",
              "      <td>93250</td>\n",
              "      <td>Villemomble</td>\n",
              "      <td>23-056603-1</td>\n",
              "      <td>2023-07-17T02:00:00+02:00</td>\n",
              "      <td>Libre service|Alimentation générale</td>\n",
              "      <td>Satisfaisant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>48.884745_2.499984</td>\n",
              "      <td>Libre service|Alimentation générale</td>\n",
              "      <td>Autres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AUBERGE DES OLIVIERS</td>\n",
              "      <td>48792443300013</td>\n",
              "      <td>ROUTE DE LA CANONICA</td>\n",
              "      <td>20290</td>\n",
              "      <td>Lucciana</td>\n",
              "      <td>23-018114-1</td>\n",
              "      <td>2023-03-08T01:00:00+01:00</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>Très satisfaisant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>42.541715_9.46286</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>Autres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LES HUITRES DES FLOTS</td>\n",
              "      <td>80481822700022</td>\n",
              "      <td>RUE DU PONT DES BERNES</td>\n",
              "      <td>50550</td>\n",
              "      <td>Saint-Vaast-la-Hougue</td>\n",
              "      <td>23-011747-1</td>\n",
              "      <td>2023-02-15T01:00:00+01:00</td>\n",
              "      <td>Purification/Expédition de coquillages</td>\n",
              "      <td>Satisfaisant</td>\n",
              "      <td>50562016</td>\n",
              "      <td>49.592002_-1.28678</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Produits de la mer et d'eau douce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>COLONNA DOMINIQUE-ANTOINE</td>\n",
              "      <td>53444598600019</td>\n",
              "      <td>Partinello</td>\n",
              "      <td>20147</td>\n",
              "      <td>Partinello</td>\n",
              "      <td>23-097885-1</td>\n",
              "      <td>2023-11-21T01:00:00+01:00</td>\n",
              "      <td>Producteur fermier</td>\n",
              "      <td>Très satisfaisant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>42.306077_8.67833</td>\n",
              "      <td>Producteur fermier</td>\n",
              "      <td>Autres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22666</th>\n",
              "      <td>AUX DELICES DE CHARENTONG</td>\n",
              "      <td>88764629700010</td>\n",
              "      <td>28 RUE DE PARIS</td>\n",
              "      <td>94220</td>\n",
              "      <td>Charenton-le-Pont</td>\n",
              "      <td>23-009992-1</td>\n",
              "      <td>2023-02-14T01:00:00+01:00</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>Satisfaisant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>48.818757_2.417811</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>Autres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22667</th>\n",
              "      <td>PICARD SURGELES</td>\n",
              "      <td>78493968801468</td>\n",
              "      <td>15 BOULEVARD BRUNE</td>\n",
              "      <td>75014</td>\n",
              "      <td>Paris 14e  Arrondissement</td>\n",
              "      <td>23-051981-1</td>\n",
              "      <td>2023-05-26T02:00:00+02:00</td>\n",
              "      <td>Libre service</td>\n",
              "      <td>Très satisfaisant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>48.825397_2.314932</td>\n",
              "      <td>Libre service</td>\n",
              "      <td>Autres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22668</th>\n",
              "      <td>CFA DE HAUTE SAONE</td>\n",
              "      <td>13002607300135</td>\n",
              "      <td>5 RUE DU TALEROT</td>\n",
              "      <td>70000</td>\n",
              "      <td>Vesoul</td>\n",
              "      <td>23-048979-1</td>\n",
              "      <td>2023-06-21T02:00:00+02:00</td>\n",
              "      <td>Découpe de viandes de volailles/lagomorphe</td>\n",
              "      <td>Très satisfaisant</td>\n",
              "      <td>70550005</td>\n",
              "      <td>47.633819_6.155241</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Viandes et produits carnés</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22669</th>\n",
              "      <td>LA FABRIQUE</td>\n",
              "      <td>49360317900042</td>\n",
              "      <td>LA FABRIQUE</td>\n",
              "      <td>08000</td>\n",
              "      <td>Charleville-Mézières</td>\n",
              "      <td>23-069730-1</td>\n",
              "      <td>2023-08-16T02:00:00+02:00</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>Très satisfaisant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>49.772441_4.720325</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>Autres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22670</th>\n",
              "      <td>SOC REGAL MAREE</td>\n",
              "      <td>32732547800037</td>\n",
              "      <td>12_ Rue Albert LAVOCAT</td>\n",
              "      <td>62200</td>\n",
              "      <td>Boulogne-sur-Mer</td>\n",
              "      <td>23-028895-1</td>\n",
              "      <td>2023-03-29T02:00:00+02:00</td>\n",
              "      <td>Mareyage et préparation de produits de la pêche</td>\n",
              "      <td>Satisfaisant</td>\n",
              "      <td>62160172</td>\n",
              "      <td>50.722087_1.593044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Produits de la mer et d'eau douce</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22671 rows × 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-949c828f-7b56-4f31-9254-95e8067c358d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-949c828f-7b56-4f31-9254-95e8067c358d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-949c828f-7b56-4f31-9254-95e8067c358d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-53b784ef-aaf8-440e-8361-7766682ea619\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-53b784ef-aaf8-440e-8361-7766682ea619')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-53b784ef-aaf8-440e-8361-7766682ea619 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 22671,\n  \"fields\": [\n    {\n      \"column\": \"APP_Libelle_etablissement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18660,\n        \"samples\": [\n          \"CAILLOCE STEPHANE\",\n          \"KORIAN LA BASTIDE\",\n          \"DISPRO FRANCE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SIRET\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20614,\n        \"samples\": [\n          \"81921337200017\",\n          \"42336669900019\",\n          \"89150190000015\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Adresse_2_UA\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19472,\n        \"samples\": [\n          \"1 RUE DESCARTES\",\n          \"34 RUE DU FG MONTMARTRE\",\n          \"52 AVENUE HENRI BARBUSSE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Code_postal\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4568,\n        \"samples\": [\n          \"33430\",\n          \"80640\",\n          \"11300\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Libelle_commune\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7065,\n        \"samples\": [\n          \"Meymac\",\n          \"Lorient\",\n          \"Taglio-Isolaccio\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Numero_inspection\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22671,\n        \"samples\": [\n          \"23-055467-1\",\n          \"23-079977-1\",\n          \"23-051411-1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date_inspection\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 303,\n        \"samples\": [\n          \"2023-05-22T02:00:00+02:00\",\n          \"2023-10-25T02:00:00+02:00\",\n          \"2023-05-24T02:00:00+02:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"APP_Libelle_activite_etablissement\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 133,\n        \"samples\": [\n          \"D\\u00e9shydratation de lait ou produits laitiers\",\n          \"Fromagerie|Chocolatier|Boulangerie-P\\u00e2tisserie|Traiteur\",\n          \"VH_ VSM_ pr\\u00e9paration viandes boucherie\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Synthese_eval_sanit\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Tr\\u00e8s satisfaisant\",\n          \"A corriger de mani\\u00e8re urgente\",\n          \"Satisfaisant\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Agrement\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4770,\n        \"samples\": [\n          \"67052501\",\n          \"14696005\",\n          \"17189033\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"geores\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19449,\n        \"samples\": [\n          \"43.724284_5.570207\",\n          \"47.100099_-0.995569\",\n          \"48.864167_2.346673\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filtre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 81,\n        \"samples\": [\n          \"Libre service|Rayon boucherie-charcuterie\",\n          \"Restaurant\",\n          \"Alimentation g\\u00e9n\\u00e9rale|Boulangerie-P\\u00e2tisserie\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ods_type_activite\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Autres\",\n          \"Produits de la mer et d'eau douce\",\n          \"Abattoirs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "obVlKlBbyDzS"
      },
      "outputs": [],
      "source": [
        "data_t = pd.read_csv('/content/Alim-Confiance/data/alim_confiance_dataset/alim_confiance_test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cities dataset it contains a list of cities in France, with departments and regions as well as GPS coordinates distrubuted by Data Gouv FR. Original dataset can be found at https://www.data.gouv.fr/en/datasets/villes-de-france/"
      ],
      "metadata": {
        "id": "1hv5u0SmY--X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mJ0R5GBtyFEg"
      },
      "outputs": [],
      "source": [
        "city = pd.read_csv('/content/Alim-Confiance/data/cities.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset below contains population numbers and amount of deaths split by age groups. I will be using the most up to date year as possible which is 2019. While I downloaded the dataset from kaggle for simplicity, the original publishing comes from https://github.com/scrouzet/covid19-incrementality\n",
        "\n",
        "The kaggle dataset can be found at https://www.kaggle.com/datasets/lperez/death-and-population-in-france-19902019"
      ],
      "metadata": {
        "id": "1S9KUZp_YQ17"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aCynsoFVyGeI"
      },
      "outputs": [],
      "source": [
        "pop = pd.read_csv('/content/Alim-Confiance/data/population_insee_dept_year_sex_age.csv', sep=';')\n",
        "death = pd.read_csv('/content/Alim-Confiance/data/INSEE_deces_2010_2019.csv', sep=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firms dataset is again taken from Kaggle but originally from National Institute of Statistics and Economic Studies (INSEE). This is the official french institute for gathering data on demographics, economics and more. This specific dataset is preprocessed to contain data on firms and number of employees in these firms per town. Kaggle link is https://www.kaggle.com/datasets/etiennelq/french-employment-by-town\n"
      ],
      "metadata": {
        "id": "bshqfTMZW5-k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PRc6_JF7yJez"
      },
      "outputs": [],
      "source": [
        "firms = pd.read_csv('/content/Alim-Confiance/data/base_etablissement_par_tranche_effectif.csv', sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lnR9C1YjyKxG"
      },
      "outputs": [],
      "source": [
        "y_map = {'A corriger de manière urgente' : 0.0,\n",
        "         'A améliorer' : 1.0,\n",
        "         'Satisfaisant' : 2.0,\n",
        "         'Très satisfaisant' : 3.0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8RvxcSAoyMZv"
      },
      "outputs": [],
      "source": [
        "data['Synthese_eval_sanit'] = data['Synthese_eval_sanit'].map(y_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcMaR8hriq4i"
      },
      "source": [
        "One of my major breakthroughs in creating my dataset was combining test and train data. This raised my accuracy in to the 67% area early on. It allowed for easy and accurate calculations of aggregate features.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EwJhGH9YyS4p"
      },
      "outputs": [],
      "source": [
        "data['trustii_id'] = 0\n",
        "data_t['Synthese_eval_sanit'] = 5\n",
        "data = pd.concat([data, data_t])\n",
        "data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are functions that will be used later on the dataframe."
      ],
      "metadata": {
        "id": "mjRO03aghKPJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YXgca63B8SYI"
      },
      "outputs": [],
      "source": [
        "def calculate_distance(x):\n",
        "        address = data[data['Numero_inspection']==str(x)].Libelle_commune.values\n",
        "        address = unidecode(address[0].lower()).replace('-', ' ')\n",
        "        post_code = data[data['Numero_inspection']==str(x)].Code_postal.values\n",
        "        match_ = difflib.get_close_matches(address, city[city['zip_code'] == post_code[0]].city_code.values, n=1, cutoff=0.1)\n",
        "        if len(match_) == 0:\n",
        "            match_ = city[city['city_code']==address]\n",
        "\n",
        "            if len(match_) >= 1:\n",
        "              new_address = difflib.get_close_matches(address, match_.city_code.values, n=1, cutoff=0.1)\n",
        "              if len(new_address) == 0:\n",
        "                return np.nan\n",
        "              match_ = match_[match_['city_code']==new_address[0]]\n",
        "\n",
        "              x_long = data[data['Numero_inspection']==str(x)].Longitude.values[0]\n",
        "              x_lat = data[data['Numero_inspection']==str(x)].Attitude.values[0]\n",
        "\n",
        "              city_lat = match_.latitude.values[0]\n",
        "              city_long = match_.longitude.values[0]\n",
        "\n",
        "              if math.isnan(x_lat):\n",
        "                  return np.nan\n",
        "              elif math.isnan(x_long):\n",
        "                  return np.nan\n",
        "              elif math.isnan(city_lat):\n",
        "                  return np.nan\n",
        "              elif math.isnan(city_long):\n",
        "                  return np.nan\n",
        "              else:\n",
        "                return hs.haversine((x_long,x_lat),(city_lat,city_long))\n",
        "\n",
        "            else:\n",
        "              return np.nan\n",
        "\n",
        "        else:\n",
        "            x_long = data[data['Numero_inspection']==str(x)].Longitude.values[0]\n",
        "            x_lat = data[data['Numero_inspection']==str(x)].Attitude.values[0]\n",
        "\n",
        "            city_lat = city[city['city_code']==match_[0]].latitude.values[0]\n",
        "            city_long = city[city['city_code']==match_[0]].longitude.values[0]\n",
        "\n",
        "            return hs.haversine((x_long,x_lat),(city_lat,city_long))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aGJQywoN-_kS"
      },
      "outputs": [],
      "source": [
        "def calculate_region(x):\n",
        "        address = data[data['Numero_inspection']==str(x)].Libelle_commune.values\n",
        "        address = unidecode(address[0].lower()).replace('-', ' ')\n",
        "        post_code = data[data['Numero_inspection']==str(x)].Code_postal.values\n",
        "        match_ = difflib.get_close_matches(address, city[city['zip_code'] == post_code[0]].city_code.values, n=1, cutoff=0.1)\n",
        "        if len(match_) == 0:\n",
        "            match_ = city[city['city_code']==address]\n",
        "            if len(match_) >= 1:\n",
        "                new_reg = difflib.get_close_matches(address, city[city['zip_code'] == post_code[0]].city_code.values, n=1, cutoff=0.1)\n",
        "                if len(new_reg) == 0:\n",
        "                  return np.nan\n",
        "                match_ = city[city['city_code']==address]\n",
        "                if len(match_) >= 1:\n",
        "                    return match_.region_name.values[0]\n",
        "                else:\n",
        "                    return 'unknown'\n",
        "        else:\n",
        "            return city[city['city_code']==match_[0]].region_name.values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Lcuds_UgjU6w"
      },
      "outputs": [],
      "source": [
        "def calculate_department(x):\n",
        "        address = data[data['Numero_inspection']==str(x)].Libelle_commune.values\n",
        "        address = unidecode(address[0].lower()).replace('-', ' ')\n",
        "        post_code = data[data['Numero_inspection']==str(x)].Code_postal.values\n",
        "        match_ = difflib.get_close_matches(address, city[city['zip_code'] == post_code[0]].city_code.values, n=1, cutoff=0.1)\n",
        "        if len(match_) == 0:\n",
        "            match_ = city[city['city_code']==address]\n",
        "            if len(match_) >= 1:\n",
        "                new_dep = difflib.get_close_matches(address, match_.city_code.values, n=1, cutoff=0.1)\n",
        "                if len(new_dep) == 0:\n",
        "                  return np.nan\n",
        "                match_ = match_[match_['city_code']==new_dep[0]]\n",
        "                return match_.department_number.values[0]\n",
        "            else:\n",
        "              return 'unknown'\n",
        "        else:\n",
        "            return city[city['city_code']==match_[0]].department_number.values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SdmhI7vv8PfA"
      },
      "outputs": [],
      "source": [
        "def fill_addresses(x):\n",
        "    bla = data[(data['SIRET']==str(x)) & (data['Adresse_2_UA'].isna())]\n",
        "    conc = bla['Adresse_2_UA'].astype(str) + '_' + bla['Longitude'].astype(str) + '_' + bla['Attitude'].astype(str)\n",
        "    conc = conc.drop_duplicates().values\n",
        "    name = bla['APP_Libelle_etablissement'].astype(str).values[0]\n",
        "    for i in range(len(conc)):\n",
        "        data.loc[(data['SIRET']==str(x)) & (data['Adresse_2_UA'].isna()), 'Adresse_2_UA'] = name + '_' + str(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiBVbOTAiq4k"
      },
      "source": [
        "Due to the spatial-temporal database the most importance features are going to be related to longitude, lattitude, postcode and date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1dffkX9Siq4k"
      },
      "outputs": [],
      "source": [
        "data['geores'] = data['geores'].apply(lambda x: x.split('_') if pd.isna(x)==False else [np.nan, np.nan])\n",
        "data2 = pd.DataFrame(data['geores'].tolist(), columns=['Longitude','Attitude'])\n",
        "data = pd.concat([data, data2], axis=1, join=\"inner\")\n",
        "data['Attitude'] = data['Attitude'].astype(np.float32)\n",
        "data['Longitude'] = data['Longitude'].astype(np.float32)\n",
        "data['Code_postal'] = data['Code_postal'].apply(lambda x: int(x) if re.search('[a-zA-Z]', x)==None else x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8mzfrKjiq4k"
      },
      "source": [
        "As there are multiple missing values for longitude and lattitude, I can use my external database to merge on postcode to give a rough location. This will not give the exact location however being in the general viscinity will help the model more than a blank value. I will also need the longitude and lattitude values for feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_qyuFbfQiq4k"
      },
      "outputs": [],
      "source": [
        "for i in data[data['Longitude'].isna()].Numero_inspection.values:\n",
        "    if len(city[city['zip_code']==data[data['Numero_inspection']==str(i)].Code_postal.values[0]]) != 0:\n",
        "        data.loc[data['Numero_inspection']==str(i), 'Longitude'] = city[city['zip_code']==data[data['Numero_inspection']==str(i)].Code_postal.values[0]].latitude.values[0]\n",
        "        data.loc[data['Numero_inspection']==str(i), 'Attitude'] = city[city['zip_code']==data[data['Numero_inspection']==str(i)].Code_postal.values[0]].longitude.values[0]\n",
        "    else:\n",
        "      address = data[data['Numero_inspection']==str(i)].Libelle_commune.values\n",
        "      address = unidecode(address[0].lower()).replace('-', ' ')\n",
        "      post_code = data[data['Numero_inspection']==str(i)].Code_postal.values\n",
        "      match_ = difflib.get_close_matches(address, city[city['zip_code'] == post_code[0]].city_code.values, n=1, cutoff=0.1)\n",
        "      if len(match_) == 0:\n",
        "         match_ = city[city['city_code'] == address]\n",
        "         if len(match_) >= 1:\n",
        "            match_ = difflib.get_close_matches(address, match_.city_code.values, n=1, cutoff=0.1)\n",
        "            data.loc[data['Numero_inspection']==str(i), 'Longitude'] = city[city['city_code']==match_[0]].latitude.values[0]\n",
        "            data.loc[data['Numero_inspection']==str(i), 'Attitude'] = city[city['city_code']==match_[0]].longitude.values[0]\n",
        "\n",
        "      else:\n",
        "        data.loc[data['Numero_inspection']==str(i), 'Longitude'] = city[city['zip_code']==match_[0]].longitude.values[0]\n",
        "        data.loc[data['Numero_inspection']==str(i), 'Attitude'] = city[city['zip_code']==match_[0]].latitude.values[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will fill in blanks for addresses by replacing all blanks with the street address and a number to create unique entries."
      ],
      "metadata": {
        "id": "Qdrkf2lr0Wku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rttTvPkFiq4k",
        "outputId": "184eea5f-6bbb-472b-ec33-ab7ccd331bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blanks Filled\n"
          ]
        }
      ],
      "source": [
        "data[data['Adresse_2_UA'].isna()].SIRET.drop_duplicates().apply(fill_addresses)\n",
        "print('Blanks Filled')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdeEUQqFiq4l"
      },
      "source": [
        "Due to the importance of location features, mapping existing values of longitude and lattitude into a 3D space will give the model more information and allow it to learn deeper features of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1pe1h53Ciq4l"
      },
      "outputs": [],
      "source": [
        "data['x'] = np.cos(data['Attitude']) * np.cos(data['Longitude'])\n",
        "data['y'] = np.cos(data['Attitude']) * np.sin(data['Longitude'])\n",
        "data['z'] = np.sin(data['Attitude'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qozgwqfjiq4l"
      },
      "source": [
        "As the data is time series based a large portion of my code is going to map values based on date. Even though the challenge itself was not time series I found most success with the accuracy when treating the data in this way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "t3Dfd6Aziq4l"
      },
      "outputs": [],
      "source": [
        "data['prev_insp'] = 0\n",
        "for i in data['SIRET']:\n",
        "  dates = data[data['SIRET']==str(i)].Date_inspection\n",
        "  dates = dates.unique()\n",
        "  if len(dates) != 1:\n",
        "    map_dates = {i:v for v, i in enumerate(sorted(dates, reverse=True))}\n",
        "    data.loc[data['SIRET'] == str(i), 'prev_insp'] = data[data['SIRET']==str(i)]['Date_inspection'].map(map_dates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHeo05umiq4m"
      },
      "source": [
        "Due to the categorical data I will one hot encode the neccesary features. In this scenario you can use either `get_dummies()` or `OneHotEncoder()`. The only benefit to `get_dummies()` is it can separate data when all combined into one column if you specificy the separator in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "E4ES4oYJiq4m"
      },
      "outputs": [],
      "source": [
        "ohe_type = ce.one_hot.OneHotEncoder(use_cat_names=True)\n",
        "data3 = ohe_type.fit_transform(data['ods_type_activite'])\n",
        "data = pd.concat([data, data3], axis=1, join=\"inner\")\n",
        "\n",
        "data6 = data['filtre'].str.get_dummies(sep='|')\n",
        "data = pd.concat([data, data6], axis=1, join=\"inner\")\n",
        "\n",
        "data.loc[data['APP_Libelle_activite_etablissement']=='_', 'APP_Libelle_activite_etablissement'] = 'unknown'\n",
        "app = data['APP_Libelle_activite_etablissement'].str.get_dummies(sep='|')\n",
        "app = app.add_prefix('app_')\n",
        "app_train_columns = app.columns\n",
        "data = pd.concat([data, app], axis=1, join=\"inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgtslHHbiq4m"
      },
      "source": [
        "Using Pandas datetime I can manipulate the date of inspection to create multiple features. I can create features for day, month, year and also day of year. Then I can create season and day of the week before encoding both columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-a3PUlLxiq4m"
      },
      "outputs": [],
      "source": [
        "data['Date_inspection'] = pd.to_datetime((data['Date_inspection'].str.split('T')).str[0], format='%Y-%m-%d')\n",
        "data['Date_inspection_days'] = data['Date_inspection'].apply(lambda x: (datetime(2024,2,7) - x).days)\n",
        "data['Date_inspection_days'] = data['Date_inspection_days'].astype(np.float32)\n",
        "\n",
        "data['year'] = pd.DatetimeIndex(data['Date_inspection']).year\n",
        "data['month'] = pd.DatetimeIndex(data['Date_inspection']).month\n",
        "data['day'] = pd.DatetimeIndex(data['Date_inspection']).day\n",
        "data['dayofyear'] = pd.DatetimeIndex(data['Date_inspection']).dayofyear\n",
        "\n",
        "data['season'] = pd.cut((data.dayofyear), [0, 80, 172, 266, 355, 366],labels=['winter', 'spring', 'summer', 'autumn', 'winter'], ordered=False)\n",
        "ohe_season = ce.one_hot.OneHotEncoder(use_cat_names=True)\n",
        "data_season = ohe_season.fit_transform(data['season'])\n",
        "data = pd.concat([data, data_season], axis=1, join=\"inner\")\n",
        "\n",
        "data['dayname'] = pd.DatetimeIndex(data['Date_inspection']).day_name()\n",
        "ohe_dayname = OrdinalEncoder(categories=[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']])\n",
        "data_dayname = ohe_dayname.fit_transform(data[['dayname']])\n",
        "data = pd.concat([data, pd.Series(data_dayname.flatten(), name='daynameenc')], axis=1, join=\"inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have created a concatatenated column of details in the dataset to create very unique entries which I will use to create new features. The original dataset has multiple inspections over multiple days and mutiple different types of inspections so I can create a column to utilise this information."
      ],
      "metadata": {
        "id": "U1Wuixu15xWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cNqLZZ-9iq4n"
      },
      "outputs": [],
      "source": [
        "data['concat_2'] = data['SIRET'].astype(str) + '_' + data['Date_inspection'].astype(str) + '_' + data['Adresse_2_UA'].astype(str)\n",
        "data['concat_3'] = data['SIRET'].astype(str) + '_' + data['APP_Libelle_activite_etablissement'].astype(str) + '_' + data['Adresse_2_UA'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihQLAj8niq4n"
      },
      "source": [
        "As I am treating this as time series I will add the time since the last inspection. The data had multiple entires on the same day with one of the only differences being the `'ods_type_activite'`. I interpretted this as different inspections on different catagories. This led me to add new features not only looking at the SIRET as a whole but to track the unique inspections within it to.\n",
        "\n",
        "This is why there is a features such as `data['prev_insp_days_unique']` as it looks at the last time this specific type of inspection was done. I do also look at the last time the inspection was done as a whole too as shown by `data['prev_insp_days']`. This is a trend I repeat in the majority of feature engineering I do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Aq9Np-NTiq4n"
      },
      "outputs": [],
      "source": [
        "data['prev_insp'] = 0\n",
        "for i in np.unique(data['SIRET'].values):\n",
        "  dates = data[data['SIRET']==str(i)].Date_inspection\n",
        "  dates = dates.unique()\n",
        "  if len(dates) != 1:\n",
        "    map_dates = {i:v for v, i in enumerate(sorted(dates, reverse=True))}\n",
        "    data.loc[data['SIRET'] == str(i), 'prev_insp'] = data[data['SIRET']==str(i)]['Date_inspection'].map(map_dates)\n",
        "\n",
        "data['prev_insp_days'] = 0\n",
        "for i in np.unique(data['SIRET'].values):\n",
        "  dates = data[data['SIRET']==str(i)].Date_inspection_days\n",
        "  dates = sorted(dates.unique(), reverse=True)\n",
        "  if len(dates) != 1:\n",
        "    order = []\n",
        "    order.append(0)\n",
        "    for j in range(len(dates)-1):\n",
        "      order.append(dates[j]-dates[j+1])\n",
        "    map_dates_days = {v:i for v, i in zip(dates,order)}\n",
        "    data.loc[data['SIRET'] == str(i), 'prev_insp_days'] = data[data['SIRET']==str(i)]['Date_inspection_days'].map(map_dates_days)\n",
        "\n",
        "data['prev_insp_days_unique'] = 0\n",
        "for i in np.unique(data['concat_3'].values):\n",
        "  dates = data[data['concat_3']==str(i)].Date_inspection_days\n",
        "  dates = sorted(dates.unique(), reverse=True)\n",
        "  if len(dates) != 1:\n",
        "    order = []\n",
        "    order.append(0)\n",
        "    for j in range(len(dates)-1):\n",
        "      order.append(dates[j]-dates[j+1])\n",
        "    map_dates_days_unique = {v:i for v, i in zip(dates,order)}\n",
        "    data.loc[data['concat_3']==str(i), 'prev_insp_days_unique'] = data[data['concat_3']==str(i)]['Date_inspection_days'].map(map_dates_days_unique)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6COFefdKiq4r"
      },
      "source": [
        "Within the data the trend of hygeine scores didnt have much variance between entries, so if it was excelent before it would most likely be excelent for the next inspection. By adding the feature `last known score`, if there has been a previous entry it will return the hygiene score. As I am treating it mostly as a time series dataset it will overlap and always return the last known score dependent on the date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4dK3WI1Oiq4r"
      },
      "outputs": [],
      "source": [
        "data['last_known_score'] = 5\n",
        "for i in np.unique(data['SIRET'].values):\n",
        "  dates = data[data['SIRET']==str(i)].sort_values(by=['Date_inspection_days']).Date_inspection_days.values\n",
        "  scores = data[data['SIRET']==str(i)].sort_values(by=['Date_inspection_days']).Synthese_eval_sanit.values\n",
        "  scores = np.insert(scores, 0, 5)\n",
        "  if len(dates) != 1:\n",
        "    map_scores = {v:i for v, i in zip(dates, scores[:-1])}\n",
        "    data.loc[data['SIRET']==str(i), 'last_known_score'] = data[data['SIRET']==str(i)]['Date_inspection_days'].map(map_scores)\n",
        "\n",
        "data['last_known_score_unique'] = 5\n",
        "for i in np.unique(data['concat_3'].values):\n",
        "  dates = data[data['concat_3']==str(i)].sort_values(by=['Date_inspection_days']).Date_inspection_days.values\n",
        "  scores = data[data['concat_3']==str(i)].sort_values(by=['Date_inspection_days']).Synthese_eval_sanit.values\n",
        "  scores = np.insert(scores, 0, 5)\n",
        "  if len(dates) != 1:\n",
        "    map_scores_unique = {v:i for v, i in zip(dates, scores[:-1])}\n",
        "    data.loc[data['concat_3']==str(i), 'last_known_score_unique'] = data[data['concat_3']==str(i)]['Date_inspection_days'].map(map_scores_unique)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCQFozj9iq4r"
      },
      "source": [
        "One of my biggest breakthroughs in gaining a significant increase in accuracy throughout this competition was when I stopped treating this as **just** a time series task. I realised that within the test set it contains SIRET numbers\n",
        "with dates in the past. This means it cannot be a time series task as you are required to predict results from entires in the past. So by introducing look-ahead bias I could increase my accuracy score and place on the leaderboard.\n",
        "\n",
        "While this does not feel like an satisfying aspect to introduce the challenge was not set up in a time series format, so adding this bias help my model in this context. Typically I would not encourage look ahead bias to be exploited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YdSqCCQCiq4s"
      },
      "outputs": [],
      "source": [
        "data['future_insp_days'] = 0\n",
        "for i in np.unique(data['SIRET'].values):\n",
        "  dates = data[data['SIRET']==str(i)].Date_inspection_days\n",
        "  dates = sorted(dates.unique(), reverse=True)\n",
        "  if len(dates) != 1:\n",
        "    order = []\n",
        "    for j in range(len(dates)-1):\n",
        "      order.append(dates[j]-dates[j+1])\n",
        "    order.append(0)\n",
        "    map_dates_days = {v:i for v, i in zip(dates,order)}\n",
        "    data.loc[data['SIRET'] == str(i), 'future_insp_days'] = data[data['SIRET']==str(i)]['Date_inspection_days'].map(map_dates_days)\n",
        "\n",
        "data['future_insp_days_unique'] = 0\n",
        "for i in np.unique(data['concat_3'].values):\n",
        "  dates = data[data['concat_3']==str(i)].Date_inspection_days\n",
        "  dates = sorted(dates.unique(), reverse=True)\n",
        "  if len(dates) != 1:\n",
        "    order = []\n",
        "    for j in range(len(dates)-1):\n",
        "      order.append(dates[j]-dates[j+1])\n",
        "    order.append(0)\n",
        "    map_dates_days_unique = {v:i for v, i in zip(dates,order)}\n",
        "    data.loc[data['concat_3']==str(i), 'future_insp_days_unique'] = data[data['concat_3']==str(i)]['Date_inspection_days'].map(map_dates_days_unique)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD5Ik5Xtiq4s"
      },
      "source": [
        "By adding future known score below it increases the look-ahead bias and consequently increases overfitting significantly. This is a trade off I made by creating the dataset this way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ynJHeQzDiq4s"
      },
      "outputs": [],
      "source": [
        "data['future_next_score'] = 5\n",
        "for i in np.unique(data['SIRET'].values):\n",
        "  dates = data[data['SIRET']==str(i)].sort_values(by=['Date_inspection_days']).Date_inspection_days.values\n",
        "  scores = data[data['SIRET']==str(i)].sort_values(by=['Date_inspection_days']).Synthese_eval_sanit.values\n",
        "  scores = np.insert(scores, 0, 5)\n",
        "  if len(dates) != 1:\n",
        "    map_scores = {v:i for v, i in zip(dates, sorted(scores[:-1], reverse=True))}\n",
        "    data.loc[data['SIRET']==str(i), 'future_next_score'] = data[data['SIRET']==str(i)]['Date_inspection_days'].map(map_scores)\n",
        "\n",
        "data['future_next_score_unique'] = 5\n",
        "for i in np.unique(data['concat_3'].values):\n",
        "  dates = data[data['concat_3']==str(i)].sort_values(by=['Date_inspection_days']).Date_inspection_days.values\n",
        "  scores = data[data['concat_3']==str(i)].sort_values(by=['Date_inspection_days']).Synthese_eval_sanit.values\n",
        "  scores = np.insert(scores, 0, 5)\n",
        "  if len(dates) != 1:\n",
        "    map_scores_unique = {v:i for v, i in zip(dates, sorted(scores[:-1], reverse=True))}\n",
        "    data.loc[data['concat_3']==str(i), 'future_next_score_unique'] = data[data['concat_3']==str(i)]['Date_inspection_days'].map(map_scores_unique)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC2Ho94Diq4s"
      },
      "source": [
        "Sometimes if there were alot of inspections it could take multiple days, and alot of correlations can be drawn between number of inspections and hygiene score. By value counting my previously concatenated column I can work out the number of inspections done per day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aEXs5HeOiq4t"
      },
      "outputs": [],
      "source": [
        "num_insp = data['concat_2'].value_counts()\n",
        "num_insp = num_insp.to_frame()\n",
        "data = data.merge(num_insp,\n",
        "                  left_on = 'concat_2',\n",
        "                  right_index = True,\n",
        "                  how ='left')\n",
        "\n",
        "data = data.rename(columns={'concat_2_y':'num_insp_per_day', 'concat_2_x':'concat_2'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1MkYrW6iq4t"
      },
      "source": [
        "Agreement column appeared to be very important. I found best results when  including the number of agreements rather than a binary column indicating presence or absence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "o5XMsooGiq4t"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([data, pd.Series((data['Agrement'].str.get_dummies(sep='|')).sum(axis=1), name='Agrement_conv')], axis=1, join='inner')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8GsCvjQiq4t"
      },
      "source": [
        "By using my earler defined functions `calculate_region` and `calculate_department`, I can use my additional datasets to add in region and departments for all entries. Then using the `calculate_distance` function it measures the distance from city center to the location of the establishment using haversine formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "eQpgp7v7iq4t"
      },
      "outputs": [],
      "source": [
        "data['region'] = data.Numero_inspection.apply(calculate_region)\n",
        "data['distance'] = data.Numero_inspection.apply(calculate_distance)\n",
        "data['department'] = data.Numero_inspection.apply(calculate_department)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUoGKKfiiq4u"
      },
      "source": [
        "Now I can encode my categorical features. I use one hot encoder for my region category, however I use ordinal encoder for `SIRET` and `APP_Libelle_etablissement` due to computational reasons, as it would create over 20,000 columns if one hot encoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "krcA2AJ23mi8"
      },
      "outputs": [],
      "source": [
        "ohe_region = ce.one_hot.OneHotEncoder(use_cat_names=True)\n",
        "data4 = ohe_region.fit_transform(data['region'])\n",
        "data = pd.concat([data, data4], axis=1, join=\"inner\")\n",
        "\n",
        "encoder_title = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = (len(data.APP_Libelle_etablissement.value_counts())+1))\n",
        "data9 = encoder_title.fit_transform(data['APP_Libelle_etablissement'].values.reshape(-1, 1))\n",
        "data = pd.concat([data, pd.Series(data9.flatten(), name='name')], axis=1, join=\"inner\")\n",
        "\n",
        "encoder_siret = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = (len(data.SIRET.value_counts())+1))\n",
        "data10 = encoder_siret.fit_transform(data['SIRET'].values.reshape(-1, 1))\n",
        "data = pd.concat([data, pd.Series(data10.flatten(), name='siret')], axis=1, join=\"inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXMNO7gZiq4u"
      },
      "source": [
        "Now I can enrich the original dataset with further open source datasets. Below I use my French population dataset to extract further features like the demographic breakdown by gender within different age groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "eGGaENFLwnEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d302ae-6c35-43e0-94b4-add24778d0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-eb08862a54de>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pop['sexe_code'] = pop['sexe_code'].map(age_map)\n"
          ]
        }
      ],
      "source": [
        "pop = pop[pop['annee']==2019]\n",
        "\n",
        "age_map = {1: 0,\n",
        "           2: 1}\n",
        "\n",
        "pop['sexe_code'] = pop['sexe_code'].map(age_map)\n",
        "pop = pop.drop(columns=['annee', 'sexe'])\n",
        "\n",
        "pop_homme = pop[pop['sexe_code']==0]\n",
        "pop_femme = pop[pop['sexe_code']==1]\n",
        "\n",
        "pop_homme = pop_homme.pivot(index=['departement_code', 'departement', 'sexe_code'],\n",
        "                            columns='classe_age',\n",
        "                            values='population').reset_index()\n",
        "pop_homme['sum_pop'] = pop_homme['00 à 19 ans'] + pop_homme['20 à 39 ans'] + pop_homme['40 à 59 ans'] + pop_homme['60 à 74 ans'] + pop_homme['75 ans et plus']\n",
        "pop_homme = pop_homme.add_prefix('pop_male_').drop(columns=['pop_male_sexe_code'])\n",
        "\n",
        "pop_femme = pop_femme.pivot(index=['departement_code', 'departement', 'sexe_code'],\n",
        "                            columns='classe_age',\n",
        "                            values='population').reset_index()\n",
        "pop_femme['sum_pop'] = pop_femme['00 à 19 ans'] + pop_femme['20 à 39 ans'] + pop_femme['40 à 59 ans'] + pop_femme['60 à 74 ans'] + pop_femme['75 ans et plus']\n",
        "pop_femme = pop_femme.add_prefix('pop_female_').drop(columns=['pop_female_sexe_code'])\n",
        "\n",
        "data = data.merge(pop_homme, left_on='department', right_on='pop_male_departement_code', how='left')\n",
        "data = data.merge(pop_femme, left_on='department', right_on='pop_female_departement_code', how='left')\n",
        "data = data.drop(columns=['pop_male_departement_code', 'pop_female_departement_code', 'pop_female_departement'])\n",
        "\n",
        "data['pop_00 à 19 ans'] = data['pop_male_00 à 19 ans'] + data['pop_female_00 à 19 ans']\n",
        "data['pop_20 à 39 ans'] = data['pop_male_20 à 39 ans'] + data['pop_female_20 à 39 ans']\n",
        "data['pop_40 à 59 ans'] = data['pop_male_40 à 59 ans'] + data['pop_female_40 à 59 ans']\n",
        "data['pop_60 à 74 ans'] = data['pop_male_60 à 74 ans'] + data['pop_female_60 à 74 ans']\n",
        "data['pop_75 ans et plus'] = data['pop_male_75 ans et plus'] + data['pop_female_75 ans et plus']\n",
        "data['pop_sum_pop'] = data['pop_male_sum_pop'] + data['pop_female_sum_pop']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBwsQEMliq4v"
      },
      "source": [
        "Now using my firms dataset I can merge my data to add new features containing number of firms per city and number of employees present within them. This will indicate the economic growth of the area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jJv4nfZxxM1P"
      },
      "outputs": [],
      "source": [
        "firms = firms.drop(columns=['CODGEO', 'LIBGEO', 'REG']).groupby(by=['DEP'], as_index=False).sum()\n",
        "data = data.merge(firms, left_on='department', right_on='DEP', how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dERiQy6Ziq4v"
      },
      "source": [
        "Next I use a dataset containing numbers of deaths in French population. This dataset allows me to add features containing amount of deaths in French cities split by sex as well as total numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QwOFwtCQETpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90001b79-ba28-41d5-88b3-579abc446150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-e61d2fa3f689>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  death_homme[\"age_group\"] = pd.cut(x=death_homme['age'], bins=[-1,19,39,49,74,200], labels=['male_00 à 19 ans','male_20 à 39 ans', 'male_40 à 59 ans', 'male_60 à 74 ans', 'male_75 ans et plus'])\n",
            "<ipython-input-35-e61d2fa3f689>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  death_femme[\"age_group\"] = pd.cut(x=death_femme['age'], bins=[-1,19,39,49,74,200], labels=['female_00 à 19 ans','female_20 à 39 ans', 'female_40 à 59 ans', 'female_60 à 74 ans', 'female_75 ans et plus'])\n"
          ]
        }
      ],
      "source": [
        "death = death[death['annee_comptabilisation']==2019]\n",
        "death = death.drop(columns=['annee_comptabilisation', 'date_deces'])\n",
        "death = death.groupby(by=['sexe','age','departement_deces'], as_index=False).sum()\n",
        "\n",
        "death['sexe'] = death['sexe'].map(age_map)\n",
        "death_homme = death[death['sexe']==0]\n",
        "death_femme = death[death['sexe']==1]\n",
        "\n",
        "death_homme[\"age_group\"] = pd.cut(x=death_homme['age'], bins=[-1,19,39,49,74,200], labels=['male_00 à 19 ans','male_20 à 39 ans', 'male_40 à 59 ans', 'male_60 à 74 ans', 'male_75 ans et plus'])\n",
        "death_homme = death_homme.drop(columns=['age', 'sexe']).groupby(by=['departement_deces','age_group'], as_index=False).sum().pivot(index=['departement_deces'],\n",
        "                                                                                                                                  columns='age_group',\n",
        "                                                                                                                                  values='nb_deces').reset_index()\n",
        "death_homme['male_sum_death'] = death_homme['male_00 à 19 ans'] + death_homme['male_20 à 39 ans'] + death_homme['male_40 à 59 ans'] + death_homme['male_60 à 74 ans'] + death_homme['male_75 ans et plus']\n",
        "death_homme = death_homme.add_prefix('death_')\n",
        "\n",
        "death_femme[\"age_group\"] = pd.cut(x=death_femme['age'], bins=[-1,19,39,49,74,200], labels=['female_00 à 19 ans','female_20 à 39 ans', 'female_40 à 59 ans', 'female_60 à 74 ans', 'female_75 ans et plus'])\n",
        "death_femme = death_femme.drop(columns=['age', 'sexe']).groupby(by=['departement_deces','age_group'], as_index=False).sum().pivot(index=['departement_deces'],\n",
        "                                                                                                                                  columns='age_group',\n",
        "                                                                                                                                  values='nb_deces').reset_index()\n",
        "death_femme['female_sum_death'] = death_femme['female_00 à 19 ans'] + death_femme['female_20 à 39 ans'] + death_femme['female_40 à 59 ans'] + death_femme['female_60 à 74 ans'] + death_femme['female_75 ans et plus']\n",
        "death_femme = death_femme.add_prefix('death_')\n",
        "\n",
        "data = data.merge(death_homme, left_on='department', right_on='death_departement_deces', how='left')\n",
        "data = data.merge(death_femme, left_on='department', right_on='death_departement_deces', how='left')\n",
        "\n",
        "data['death_00 à 19 ans'] = data['death_male_00 à 19 ans'] + data['death_female_00 à 19 ans']\n",
        "data['death_20 à 39 ans'] = data['death_male_20 à 39 ans'] + data['death_female_20 à 39 ans']\n",
        "data['death_40 à 59 ans'] = data['death_male_40 à 59 ans'] + data['death_female_40 à 59 ans']\n",
        "data['death_60 à 74 ans'] = data['death_male_60 à 74 ans'] + data['death_female_60 à 74 ans']\n",
        "data['death_75 ans et plus'] = data['death_male_75 ans et plus'] + data['death_female_75 ans et plus']\n",
        "data['death_sum_pop'] = data['death_male_sum_death'] + data['death_female_sum_death']\n",
        "\n",
        "data.loc[data['pop_male_departement'].isna(), 'pop_male_departement'] = 'unknown'\n",
        "\n",
        "ohe_dep = ce.one_hot.OneHotEncoder(use_cat_names=True)\n",
        "dep = ohe_dep.fit_transform(data['pop_male_departement'])\n",
        "data = pd.concat([data, dep], axis=1, join=\"inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6fFWyO7iq5U"
      },
      "source": [
        "I generated new aggregated metrics for average hygiene scores, categorized by region and department. I experimented alot with these features, testing averaging different columns and even adding in standard deviations and variance. I found adding these features made my model overfit a lot so ultimately decided to remove them from my dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "n9Xa_BXviq5U"
      },
      "outputs": [],
      "source": [
        "region_avg = data[data['Synthese_eval_sanit']!=5].groupby(['region'],as_index=False)['Synthese_eval_sanit'].mean().rename(columns={'Synthese_eval_sanit':'region_avg'})\n",
        "data = data.merge(region_avg, left_on='region', right_on='region', how='left')\n",
        "dep_avg = data[data['Synthese_eval_sanit']!=5].groupby(['pop_male_departement'],as_index=False)['Synthese_eval_sanit'].mean().rename(columns={'Synthese_eval_sanit':'dep_avg'})\n",
        "data = data.merge(dep_avg, left_on='pop_male_departement', right_on='pop_male_departement', how='left')\n",
        "\n",
        "pcode = data[data['Synthese_eval_sanit']!=5].groupby(['Code_postal'],as_index=False).size()\n",
        "pcode_values = pcode[pcode['size']>=2].Code_postal.values\n",
        "post_avg = data[(data['Code_postal'].isin(pcode_values)) & (data['Synthese_eval_sanit']!=5)].groupby(['Code_postal'],as_index=False)['Synthese_eval_sanit'].mean().rename(columns={'Synthese_eval_sanit':'post_avg'})\n",
        "data = data.merge(post_avg, left_on='Code_postal', right_on='Code_postal', how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enchance the learning process and provide additional context I will add 3 further datasets from INSEE. The SIRET number was essential to creating a successful model as it gave me access to detailed information on specific business addresses, activity codes, legal forms, and more. Below I use establishments stock file (active and closed), legal stock file and establishment succession links stock file."
      ],
      "metadata": {
        "id": "FCYPecF_WEV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SIRET Establishments Stock File**"
      ],
      "metadata": {
        "id": "lgi7tGHAzeqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the size of the SIRET dataset, I am using `read_csv()` by Polars to load into memory. If I was to load it using Pandas my session would crash due to the size. Polars efficiently reads large datasets by leveraging a lazy evaluation engine and parallel processing, optimizing performance for big data tasks.\n",
        "\n",
        "After it is loaded I will filter to only the have relevant data for all SIRET number in my dataset. Then I will convert back to Pandas as I prefer the code syntax and familiarity of the module over Polars."
      ],
      "metadata": {
        "id": "H-RpgxL-w6D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sir = pl.read_csv('/content/Alim-Confiance/data/StockEtablissement_utf8_SIRET.csv', low_memory=True, infer_schema_length=10000, ignore_errors=True, null_values=['[ND]'])"
      ],
      "metadata": {
        "id": "k5srYMKrzeq3"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n =  data[data['SIRET'].str.contains('[a-zA-Z]')==False]\n",
        "siret_n['SIRET'] = siret_n['SIRET'].astype(int)\n",
        "siret_n = pl.DataFrame(data = siret_n.SIRET.values, schema=['SIRET'])\n",
        "siret_n = siret_n.join(sir, left_on='SIRET', right_on='SIRET', how='left')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c844882a-663d-4f19-fd5d-f709158b7aff",
        "id": "GyK29_zYzeq3"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-174-8c4f9b24c483>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  siret_n['SIRET'] = siret_n['SIRET'].astype(int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "siret_miss =  data[data['SIRET'].str.contains('[a-zA-Z]')==True]\n",
        "siret_miss = siret_miss.SIRET"
      ],
      "metadata": {
        "id": "En9VBvxFzeq4"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n = siret_n.to_pandas()"
      ],
      "metadata": {
        "id": "9c3BlI8Dzeq4"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n = pd.concat([siret_n, pd.DataFrame(siret_miss, columns=siret_n.columns)], axis=0)"
      ],
      "metadata": {
        "id": "0goOakfAzeq4"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n = siret_n.drop_duplicates(subset=['SIRET'])\n",
        "siret_n.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "rThmL_l1zeq4"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the SIRET Establishments Stock File we are able to gain many addtional features. Most notably Main Establishment Activity Classification, Establishment Creation Date and Establishment Number of Employees.\n",
        "\n",
        "The features are typically either binary or categorical, so alot of my time was spent setting values to 0/1 or using ```get_dummies()``` to encode the categorical features.\n",
        "\n"
      ],
      "metadata": {
        "id": "5HGGdGBiqm-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['activitePrincipaleEtablissement'] = siret_n['activitePrincipaleEtablissement'].fillna('princ_unknown')\n",
        "princ = siret_n['activitePrincipaleEtablissement'].str.get_dummies(sep='|')\n",
        "siret_n = pd.concat([siret_n, princ], axis=1, join=\"inner\")"
      ],
      "metadata": {
        "id": "jiU5AiNpzeq4"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['activitePrincipaleRegistreMetiersEtablissement'] = siret_n['activitePrincipaleRegistreMetiersEtablissement'].fillna('princ_reg_unknown')\n",
        "princ_reg = siret_n['activitePrincipaleRegistreMetiersEtablissement'].str.get_dummies(sep='|')\n",
        "siret_n = pd.concat([siret_n, princ_reg], axis=1, join=\"inner\")"
      ],
      "metadata": {
        "id": "RTXvHorizeq4"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['caractereEmployeurEtablissement'] = siret_n['caractereEmployeurEtablissement'].fillna('N')\n",
        "changement_map = {'O':1,\n",
        "                  'N':0}\n",
        "\n",
        "siret_n['caractereEmployeurEtablissement'] = siret_n['caractereEmployeurEtablissement'].map(changement_map)"
      ],
      "metadata": {
        "id": "SRQTM_V4zeq4"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['codeCommuneEtablissement'] = siret_n['codeCommuneEtablissement'].fillna(0)\n",
        "encoder_code = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value=np.nan)\n",
        "code = encoder_code.fit_transform(siret_n['codeCommuneEtablissement'].values.reshape(-1, 1))\n",
        "siret_n = pd.concat([siret_n, pd.Series(code.flatten(), name='code')], axis=1, join=\"inner\")"
      ],
      "metadata": {
        "id": "OGFPU3jRzeq5"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['codePaysEtrangerEtablissement'] = siret_n['codePaysEtrangerEtablissement'].apply(lambda x: 0 if pd.isnull(x) else 1)"
      ],
      "metadata": {
        "id": "vbKOjllwzeq5"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['dateDebut'] = siret_n['dateDebut'].fillna(datetime(2024,2,7))\n",
        "siret_n['dateDebut'] = pd.to_datetime(siret_n['dateDebut'], format='%Y-%m-%d')\n",
        "siret_n['dateDebut'] = siret_n['dateDebut'].apply(lambda x: (datetime(2024,2,7) - x).days)"
      ],
      "metadata": {
        "id": "cTzmzd36zeq5"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['dateCreationEtablissement'] = siret_n['dateCreationEtablissement'].fillna(datetime(2024,2,7))\n",
        "siret_n['dateCreationEtablissement'] = pd.to_datetime(siret_n['dateCreationEtablissement'], format='%Y-%m-%d')\n",
        "siret_n['dateCreationEtablissement'] = siret_n['dateCreationEtablissement'].apply(lambda x: (datetime(2024,2,7) - x).days)"
      ],
      "metadata": {
        "id": "dEm2yX1Rzeq5"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['dateDernierTraitementEtablissement'] = siret_n['dateDernierTraitementEtablissement'].str.split('T').str[0]\n",
        "siret_n['dateDernierTraitementEtablissement'] = siret_n['dateDernierTraitementEtablissement'].fillna(datetime(2024,2,7))\n",
        "siret_n['dateDernierTraitementEtablissement'] = pd.to_datetime(siret_n['dateDernierTraitementEtablissement'], format='%Y-%m-%d')\n",
        "siret_n['dateDernierTraitementEtablissement'] = siret_n['dateDernierTraitementEtablissement'].apply(lambda x: (datetime(2024,2,7) - x).days)"
      ],
      "metadata": {
        "id": "vh588ZOszeq5"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['denominationUsuelleEtablissement'] = siret_n['denominationUsuelleEtablissement'].fillna(\"'\")\n",
        "encoder_denomination = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value=np.nan)\n",
        "deno = encoder_denomination.fit_transform(siret_n['denominationUsuelleEtablissement'].values.reshape(-1, 1))\n",
        "siret_n = pd.concat([siret_n, pd.Series(deno.flatten(), name='deno')], axis=1, join=\"inner\").drop(columns=['denominationUsuelleEtablissement'])"
      ],
      "metadata": {
        "id": "21rnPswrzeq5"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['distributionSpecialeEtablissement'] = siret_n['distributionSpecialeEtablissement'].fillna('dist_unknown')\n",
        "siret_n.loc[siret_n['distributionSpecialeEtablissement']=='      ', 'distributionSpecialeEtablissement'] = '0_'\n",
        "dist = siret_n['distributionSpecialeEtablissement'].str.get_dummies(sep='|')\n",
        "siret_n = pd.concat([siret_n, dist], axis=1, join=\"inner\")"
      ],
      "metadata": {
        "id": "df6jaJDKzeq5"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['enseigne1Etablissement'] = siret_n['enseigne1Etablissement'].fillna(\"'\")\n",
        "encoder_etab = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value=np.nan)\n",
        "etab = encoder_etab.fit_transform(siret_n['enseigne1Etablissement'].values.reshape(-1, 1))\n",
        "siret_n = pd.concat([siret_n, pd.Series(etab.flatten(), name='etab')], axis=1, join=\"inner\").drop(columns=['enseigne1Etablissement'])"
      ],
      "metadata": {
        "id": "95_geEAyzeq5"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['etablissementSiege'] = siret_n['etablissementSiege'].fillna(True)\n",
        "boolean_map = {True:1,\n",
        "               False:0}\n",
        "\n",
        "siret_n['etablissementSiege'] = siret_n['etablissementSiege'].map(boolean_map)"
      ],
      "metadata": {
        "id": "bdB7hbqdzeq5"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['etatAdministratifEtablissement'] = siret_n['etatAdministratifEtablissement'].fillna('F')\n",
        "admin_map = {'A':1,\n",
        "             'F':0}\n",
        "\n",
        "siret_n['etatAdministratifEtablissement'] = siret_n['etatAdministratifEtablissement'].map(admin_map)"
      ],
      "metadata": {
        "id": "wZasl9Zdzeq5"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['indiceRepetitionEtablissement'] = siret_n['indiceRepetitionEtablissement'].fillna('indi_unknown')\n",
        "siret_n.loc[siret_n['indiceRepetitionEtablissement']==' ', 'indiceRepetitionEtablissement'] = 'indi_unknown'\n",
        "indi = siret_n['indiceRepetitionEtablissement'].str.get_dummies(sep='|')\n",
        "siret_n = pd.concat([siret_n, indi], axis=1, join=\"inner\")"
      ],
      "metadata": {
        "id": "APkpT2WVzeq6"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['libelleCedexEtablissement'] = siret_n['libelleCedexEtablissement'].fillna('cedex_unknown')\n",
        "cedex = siret_n['libelleCedexEtablissement'].str.get_dummies(sep='|')\n",
        "siret_n = pd.concat([siret_n, cedex], axis=1, join=\"inner\")"
      ],
      "metadata": {
        "id": "GUh_oi-6zeq6"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['libelleCommuneEtrangerEtablissement'] = siret_n['libelleCommuneEtrangerEtablissement'].apply(lambda x: 0 if pd.isnull(x) else 1)"
      ],
      "metadata": {
        "id": "O4FdEmwRzeq6"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['libellePaysEtrangerEtablissement'] = siret_n['libellePaysEtrangerEtablissement'].apply(lambda x: 0 if pd.isnull(x) else 1)"
      ],
      "metadata": {
        "id": "gLWCT2kYzeq6"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['nic'] = siret_n['nic'].fillna(0)"
      ],
      "metadata": {
        "id": "yNu3_pVMzeq6"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['nombrePeriodesEtablissement'] = siret_n['nombrePeriodesEtablissement'].fillna(0)"
      ],
      "metadata": {
        "id": "_o1SlINvzeq6"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['nomenclatureActivitePrincipaleEtablissement'] = siret_n['nomenclatureActivitePrincipaleEtablissement'].fillna('NAFRev2')\n",
        "nomen = siret_n['nomenclatureActivitePrincipaleEtablissement'].str.get_dummies(sep='|')\n",
        "siret_n = pd.concat([siret_n, nomen], axis=1, join=\"inner\")"
      ],
      "metadata": {
        "id": "fckdkdLQzeq6"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['siren'] = siret_n['siren'].fillna(0)"
      ],
      "metadata": {
        "id": "XOnVdBVVzeq6"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['statutDiffusionEtablissement'] = siret_n['statutDiffusionEtablissement'].fillna('O')\n",
        "diff_map = {'P':1,\n",
        "            'O':0}\n",
        "\n",
        "siret_n['statutDiffusionEtablissement'] = siret_n['statutDiffusionEtablissement'].map(diff_map)"
      ],
      "metadata": {
        "id": "WcjJZzIyzeq6"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['trancheEffectifsEtablissement'] = siret_n['trancheEffectifsEtablissement'].fillna(\"UNK\")\n",
        "encoder_tranche = OrdinalEncoder(categories=[['UNK', 'NN', '00', '01', '02', '03', '11', '12', '21', '22', '31', '32', '41', '42', '51', '52', '53']])\n",
        "tranche = encoder_tranche.fit_transform(siret_n['trancheEffectifsEtablissement'].values.reshape(-1, 1))\n",
        "siret_n = pd.concat([siret_n, pd.Series(tranche.flatten(), name='tranche')], axis=1, join=\"inner\").drop(columns=['trancheEffectifsEtablissement'])"
      ],
      "metadata": {
        "id": "NZ9GN2fNzeq6"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n['typeVoieEtablissement'] = siret_n['typeVoieEtablissement'].fillna(\"voie_UNK\")\n",
        "voie = siret_n['typeVoieEtablissement'].str.get_dummies(sep='|')\n",
        "siret_n = pd.concat([siret_n, voie], axis=1, join=\"inner\")"
      ],
      "metadata": {
        "id": "9f2BEWutzeq6"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once all features are encoded I will remove the remaining categorical features that are irrelevant to my task."
      ],
      "metadata": {
        "id": "D1Q0GUURaiP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "siren_dtypes = pd.DataFrame(siret_n.dtypes)\n",
        "drop_cols = siren_dtypes[siren_dtypes[0]=='object'].index"
      ],
      "metadata": {
        "id": "wHDGOf76zeq6"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "na_vals = pd.DataFrame(siret_n.isna().sum())\n",
        "add_drop_cols = na_vals[na_vals[0]!=0].index\n",
        "drop_cols = np.append(drop_cols, add_drop_cols)\n",
        "drop_cols = np.delete(drop_cols, 0)"
      ],
      "metadata": {
        "id": "QUICvaVKzeq6"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n = siret_n.drop(columns=drop_cols)"
      ],
      "metadata": {
        "id": "LtAlMmxpzeq6"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As some column names are shared over the SIRET files, I have added a prefix to differentiate them."
      ],
      "metadata": {
        "id": "_opdjyy4ar0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "siret_n = siret_n.add_prefix('n_').rename(columns={'n_SIRET':'SIRET', 'n_siren':'siren', 'n_nic':'nic'})"
      ],
      "metadata": {
        "id": "qcV_MYJaNJ8K"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SIRET Legal Stock File**"
      ],
      "metadata": {
        "id": "aJ1jvHld0T1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again due to the size of the files I will be using polars then converting to pandas once necesssary."
      ],
      "metadata": {
        "id": "SHJGAddkbVtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "leg = pl.read_csv('/content/Alim-Confiance/data/StockUniteLegale_utf8_SIRET.csv', low_memory=True, infer_schema_length=10000, ignore_errors=True, null_values=['[ND]'])"
      ],
      "metadata": {
        "id": "g1V9geoC0T13"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l =  siret_n[['siren']]\n",
        "siret_l = pl.DataFrame(data = np.unique(siret_n.siren.values), schema=['siren'])\n",
        "siret_l = siret_l.with_columns(pl.col('siren').cast(pl.Int64))\n",
        "siret_l = siret_l.join(leg, left_on='siren', right_on='siren', how='left')"
      ],
      "metadata": {
        "id": "KkDbOi5U0T13"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l = siret_l.to_pandas()"
      ],
      "metadata": {
        "id": "Gi0g0Cbe0T13"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main features I am able to extract from the SIRET legal stock file are the Main Activity of the Legal Entity, Legal Entity Creation Date and Legal Entity Workforce Employee Count. Again the overwhelming majority are either categorical or binary features, so I convert them where necessary below."
      ],
      "metadata": {
        "id": "m4KUkcsccVEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['caractereEmployeurUniteLegale'] = siret_l['caractereEmployeurUniteLegale'].fillna('O')\n",
        "unite_map = {'O':1,\n",
        "             'N':0}\n",
        "\n",
        "siret_l['caractereEmployeurUniteLegale'] = siret_l['caractereEmployeurUniteLegale'].map(unite_map)"
      ],
      "metadata": {
        "id": "Zgu7IjSm0T13"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['categorieEntreprise'] = siret_l['categorieEntreprise'].fillna('UNK')\n",
        "entre_map = {'GE':3,\n",
        "             'ETI':2,\n",
        "             'PME':1,\n",
        "             'UNK':0}\n",
        "\n",
        "siret_l['categorieEntreprise'] = siret_l['categorieEntreprise'].map(entre_map)"
      ],
      "metadata": {
        "id": "xAKigyex0T14"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['categorieJuridiqueUniteLegale'] = siret_l['categorieJuridiqueUniteLegale'].fillna(5710.0)\n",
        "siret_l['categorieJuridiqueUniteLegale'] = siret_l['categorieJuridiqueUniteLegale'].astype('str')\n",
        "juri = siret_l['categorieJuridiqueUniteLegale'].str.get_dummies(sep='|')\n",
        "siret_l = pd.concat([siret_l, juri], axis=1, join=\"inner\")"
      ],
      "metadata": {
        "id": "9OPa-wzR0T14"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['dateCreationUniteLegale'] = siret_l['dateCreationUniteLegale'].fillna(datetime(1900,1,1))\n",
        "siret_l['dateCreationUniteLegale'] = pd.to_datetime(siret_l['dateCreationUniteLegale'], format='%Y-%m-%d')\n",
        "siret_l['dateCreationUniteLegale'] = siret_l['dateCreationUniteLegale'].apply(lambda x: (datetime(2024,2,7) - x).days)"
      ],
      "metadata": {
        "id": "QW7IN6-80T14"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['dateDebut'] = siret_l['dateDebut'].fillna(datetime(2024,2,7))\n",
        "siret_l['dateDebut'] = pd.to_datetime(siret_l['dateDebut'], format='%Y-%m-%d', errors='coerce')\n",
        "siret_l['dateDebut'] = siret_l['dateDebut'].fillna(datetime(2024,2,7))\n",
        "siret_l['dateDebut'] = siret_l['dateDebut'].apply(lambda x: (datetime(2024,2,7) - x).days)"
      ],
      "metadata": {
        "id": "SpGSQS110T14"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['dateDernierTraitementUniteLegale'] = pd.to_datetime((siret_l['dateDernierTraitementUniteLegale'].str.split('T')).str[0], format='%Y-%m-%d')\n",
        "siret_l['dateDernierTraitementUniteLegale'] = siret_l['dateDernierTraitementUniteLegale'].fillna(datetime(2024,2,7))\n",
        "siret_l['dateDernierTraitementUniteLegale'] = pd.to_datetime(siret_l['dateDernierTraitementUniteLegale'], format='%Y-%m-%d', errors='coerce')\n",
        "siret_l['dateDernierTraitementUniteLegale'] = siret_l['dateDernierTraitementUniteLegale'].apply(lambda x: (datetime(2024,2,7) - x).days)"
      ],
      "metadata": {
        "id": "5LvOeNJM0T14"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['denominationUniteLegale'] = siret_l['denominationUniteLegale'].fillna(\"'\")\n",
        "encoder_deno_leg = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value=np.nan)\n",
        "deno_leg = encoder_deno_leg.fit_transform(siret_l['denominationUniteLegale'].values.reshape(-1, 1))\n",
        "siret_l = pd.concat([siret_l, pd.Series(deno_leg.flatten(), name='deno_leg')], axis=1, join=\"inner\").drop(columns=['denominationUniteLegale'])"
      ],
      "metadata": {
        "id": "dkTYQmqh0T14"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['denominationUsuelle1UniteLegale'] = siret_l['denominationUsuelle1UniteLegale'].fillna(\"'\")\n",
        "encoder_deno_usue = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value=np.nan)\n",
        "deno_usue = encoder_deno_usue.fit_transform(siret_l['denominationUsuelle1UniteLegale'].values.reshape(-1, 1))\n",
        "siret_l = pd.concat([siret_l, pd.Series(deno_usue.flatten(), name='deno_usue')], axis=1, join=\"inner\").drop(columns=['denominationUsuelle1UniteLegale'])"
      ],
      "metadata": {
        "id": "a9n_f1_v0T14"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['economieSocialeSolidaireUniteLegale'] = siret_l['economieSocialeSolidaireUniteLegale'].fillna('UNK')\n",
        "economie_map = {'O':2,\n",
        "                'N':1,\n",
        "                'UNK':0}\n",
        "\n",
        "siret_l['economieSocialeSolidaireUniteLegale'] = siret_l['economieSocialeSolidaireUniteLegale'].map(economie_map)"
      ],
      "metadata": {
        "id": "Wl3ZEMdH0T14"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['etatAdministratifUniteLegale'] = siret_l['etatAdministratifUniteLegale'].fillna('A')\n",
        "admin_leg_map = {'A':1,\n",
        "                 'C':0}\n",
        "\n",
        "siret_l['etatAdministratifUniteLegale'] = siret_l['etatAdministratifUniteLegale'].map(admin_leg_map)"
      ],
      "metadata": {
        "id": "6wf48AA60T14"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['identifiantAssociationUniteLegale'] = siret_l['identifiantAssociationUniteLegale'].apply(lambda x: 0 if pd.isnull(x) else 1)"
      ],
      "metadata": {
        "id": "DPK5JYL20T14"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['nicSiegeUniteLegale'] = siret_l['nicSiegeUniteLegale'].fillna(0)"
      ],
      "metadata": {
        "id": "5qcvL9K70T14"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['nombrePeriodesUniteLegale'] = siret_l['nombrePeriodesUniteLegale'].fillna(0)"
      ],
      "metadata": {
        "id": "4gvS37Pg0T14"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['sexeUniteLegale'] = siret_l['sexeUniteLegale'].fillna('UNK')\n",
        "sexe_leg_map = {'M':2,\n",
        "                'F':1,\n",
        "                'UNK':0}\n",
        "\n",
        "siret_l['sexeUniteLegale'] = siret_l['sexeUniteLegale'].map(sexe_leg_map)"
      ],
      "metadata": {
        "id": "29fABHWZ0T14"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['societeMissionUniteLegale'] = siret_l['societeMissionUniteLegale'].fillna('UNK')\n",
        "miss_leg_map = {'N':2,\n",
        "                'O':1,\n",
        "                'UNK':0}\n",
        "\n",
        "siret_l['societeMissionUniteLegale'] = siret_l['societeMissionUniteLegale'].map(miss_leg_map)"
      ],
      "metadata": {
        "id": "E9b9fYsQ0T14"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['statutDiffusionUniteLegale'] = siret_l['statutDiffusionUniteLegale'].fillna('O')\n",
        "stat_leg_map = {'O':0,\n",
        "                'P':1}\n",
        "\n",
        "siret_l['statutDiffusionUniteLegale'] = siret_l['statutDiffusionUniteLegale'].map(stat_leg_map)"
      ],
      "metadata": {
        "id": "0ZbOkaim0T15"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l['trancheEffectifsUniteLegale'] = siret_l['trancheEffectifsUniteLegale'].fillna(\"UNK\")\n",
        "encoder_leg_tranche = OrdinalEncoder(categories=[['UNK', 'NN', '00', '01', '02', '03', '11', '12', '21', '22', '31', '32', '41', '42', '51', '52', '53']])\n",
        "tranche_leg = encoder_leg_tranche.fit_transform(siret_l['trancheEffectifsUniteLegale'].values.reshape(-1, 1))\n",
        "siret_l = pd.concat([siret_l, pd.Series(tranche_leg.flatten(), name='tranche_leg')], axis=1, join=\"inner\").drop(columns=['trancheEffectifsUniteLegale'])"
      ],
      "metadata": {
        "id": "GUtyKWL-0T15"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtpyes_siret_l = pd.DataFrame(siret_l.dtypes)\n",
        "drop_cols = dtpyes_siret_l[dtpyes_siret_l[0]=='object'].index"
      ],
      "metadata": {
        "id": "wPD6WujJ0T15"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l = siret_l.drop(columns=np.append(drop_cols, ['anneeCategorieEntreprise', 'anneeEffectifsUniteLegale']))"
      ],
      "metadata": {
        "id": "tTStiEJ00T15"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_l = siret_l.add_prefix('l_').rename(columns={'l_siren':'siren'})"
      ],
      "metadata": {
        "id": "4mGsmwYJSPPE"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SIRET Succession Links Stock File**"
      ],
      "metadata": {
        "id": "GbQUdBso0jx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again same as other two Stock Files I am able to use polars then pandas."
      ],
      "metadata": {
        "id": "YkLRuHVVec64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sire = pl.read_csv('/content/Alim-Confiance/data/StockEtablissementLiensSuccession_utf8_SIRET.csv', low_memory=True, infer_schema_length=10000, ignore_errors=True, null_values=['[ND]'])"
      ],
      "metadata": {
        "id": "OIHev1toFaRJ"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s =  data[data['SIRET'].str.contains('[a-zA-Z]')==False]\n",
        "siret_s['SIRET'] = siret_s['SIRET'].astype(int)\n",
        "siret_s = pl.DataFrame(data = np.unique(siret_s.SIRET.values), schema=['siret'])\n",
        "siret_s = siret_s.with_columns(pl.col('siret').cast(pl.Int64))\n",
        "siret_s = siret_s.join(sire, left_on='siret', right_on='siretEtablissementSuccesseur', how='left')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61f6d40-fe71-47fd-bea8-06fbad54c4d8",
        "id": "RfqCXx9n0jx6"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-289-5808bac76d59>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  siret_s['SIRET'] = siret_s['SIRET'].astype(int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s = siret_s.to_pandas()"
      ],
      "metadata": {
        "id": "7n8gF6HH0jx7"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_miss =  data[data['SIRET'].str.contains('[a-zA-Z]')==True]\n",
        "siret_miss = siret_miss.SIRET.rename('siret')\n",
        "\n",
        "siret_s = pd.concat([siret_s, pd.DataFrame(siret_miss, columns=siret_s.columns)], axis=0)"
      ],
      "metadata": {
        "id": "gLTNPgMl0jx7"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s = siret_s.drop_duplicates(subset=['siret']).rename(columns={'siret':'SIRET'})\n",
        "siret_s.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "aHbabRpp0jx7"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main use of this stock file is to find if there predecessor or successor for any SIREN number and the creation date. First I will pre-process predecessor data."
      ],
      "metadata": {
        "id": "tdDM6HoWemqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s['dateLienSuccession'] = siret_s['dateLienSuccession'].fillna(datetime(2024,2,7))\n",
        "siret_s['dateLienSuccession'] = pd.to_datetime(siret_s['dateLienSuccession'], format='%Y-%m-%d', errors='coerce')\n",
        "siret_s['dateLienSuccession'] = siret_s['dateLienSuccession'].apply(lambda x: (datetime(2024,2,7) - x).days)"
      ],
      "metadata": {
        "id": "hqcXUoQt0jx7"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s['transfertSiege'] = siret_s['transfertSiege'].fillna('UNK')\n",
        "trans_suc_map = {True:2,\n",
        "                 False:1,\n",
        "                 'UNK':0}\n",
        "\n",
        "siret_s['transfertSiege'] = siret_s['transfertSiege'].map(trans_suc_map)"
      ],
      "metadata": {
        "id": "lX8c1uYw0jx7"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s['continuiteEconomique'] = siret_s['continuiteEconomique'].fillna('UNK')\n",
        "econo_suc_map = {True:2,\n",
        "                 False:1,\n",
        "                 'UNK':0}\n",
        "\n",
        "siret_s['continuiteEconomique'] = siret_s['continuiteEconomique'].map(econo_suc_map)"
      ],
      "metadata": {
        "id": "LmRIsyda0jx7"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s['dateDernierTraitementLienSuccession'] = pd.to_datetime((siret_s['dateDernierTraitementLienSuccession'].str.split('T')).str[0], format='%Y-%m-%d')\n",
        "siret_s['dateDernierTraitementLienSuccession'] = siret_s['dateDernierTraitementLienSuccession'].fillna(datetime(2024,2,7))\n",
        "siret_s['dateDernierTraitementLienSuccession'] = siret_s['dateDernierTraitementLienSuccession'].apply(lambda x: (datetime(2024,2,7) - x).days)"
      ],
      "metadata": {
        "id": "DZ-hIsy_0jx7"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s['siretEtablissementPredecesseur'] = siret_s['siretEtablissementPredecesseur'].fillna(0)"
      ],
      "metadata": {
        "id": "ahyzFmfn0jx7"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s['has_pred'] = siret_s['siretEtablissementPredecesseur'].apply(lambda x: 0 if int(x)==0 else 1)"
      ],
      "metadata": {
        "id": "3Ht3yn5I0jx8"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s = siret_s.add_prefix('s_').rename(columns={'s_SIRET':'SIRET'})"
      ],
      "metadata": {
        "id": "xZMbbxGlSeu4"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s['s_siretEtablissementPredecesseur'] = siret_s['s_siretEtablissementPredecesseur'].astype(str)"
      ],
      "metadata": {
        "id": "7kk_ublLYcFo"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I can repeat the process for successor data."
      ],
      "metadata": {
        "id": "abS4tXRgf03y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2 =  data[data['SIRET'].str.contains('[a-zA-Z]')==False]\n",
        "siret_s2['SIRET'] = siret_s2['SIRET'].astype(int)\n",
        "siret_s2 = pl.DataFrame(data = np.unique(siret_s2.SIRET.values), schema=['siret'])\n",
        "siret_s2 = siret_s2.with_columns(pl.col('siret').cast(pl.Int64))\n",
        "siret_s2 = siret_s2.join(sire, left_on='siret', right_on='siretEtablissementPredecesseur', how='left')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a40009f-94e5-467f-d4de-a0204f2e6429",
        "id": "lrg0Mw7m0jx8"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-243-b9dfdfa43ec8>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  siret_s2['SIRET'] = siret_s2['SIRET'].astype(int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2 = siret_s2.to_pandas()"
      ],
      "metadata": {
        "id": "1Z8XitUt0jx9"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_miss =  data[data['SIRET'].str.contains('[a-zA-Z]')==True]\n",
        "siret_miss = siret_miss.SIRET.rename('siret')\n",
        "\n",
        "siret_s2 = pd.concat([siret_s2, pd.DataFrame(siret_miss, columns=siret_s2.columns)], axis=0)"
      ],
      "metadata": {
        "id": "nZLHit7c0jx9"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2 = siret_s2.drop_duplicates(subset=['siret']).rename(columns={'siret':'SIRET'})\n",
        "siret_s2.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "qZrN49K10jx9"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2['dateLienSuccession'] = siret_s2['dateLienSuccession'].fillna(datetime(2024,2,7))\n",
        "siret_s2['dateLienSuccession'] = pd.to_datetime(siret_s2['dateLienSuccession'], format='%Y-%m-%d', errors='coerce')\n",
        "siret_s2['dateLienSuccession'] = siret_s2['dateLienSuccession'].apply(lambda x: (datetime(2024,2,7) - x).days)"
      ],
      "metadata": {
        "id": "U631-yIA0jx-"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2['transfertSiege'] = siret_s2['transfertSiege'].fillna('UNK')\n",
        "trans_suc_map = {True:2,\n",
        "                 False:1,\n",
        "                 'UNK':0}\n",
        "\n",
        "siret_s2['transfertSiege'] = siret_s2['transfertSiege'].map(trans_suc_map)"
      ],
      "metadata": {
        "id": "lgu-KBYe0jx-"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2['continuiteEconomique'] = siret_s2['continuiteEconomique'].fillna('UNK')\n",
        "econo_suc_map = {True:2,\n",
        "                 False:1,\n",
        "                 'UNK':0}\n",
        "\n",
        "siret_s2['continuiteEconomique'] = siret_s2['continuiteEconomique'].map(econo_suc_map)"
      ],
      "metadata": {
        "id": "ykhz-mmH0jx-"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2['dateDernierTraitementLienSuccession'] = pd.to_datetime((siret_s2['dateDernierTraitementLienSuccession'].str.split('T')).str[0], format='%Y-%m-%d')\n",
        "siret_s2['dateDernierTraitementLienSuccession'] = siret_s2['dateDernierTraitementLienSuccession'].fillna(datetime(2024,2,7))\n",
        "siret_s2['dateDernierTraitementLienSuccession'] = siret_s2['dateDernierTraitementLienSuccession'].apply(lambda x: (datetime(2024,2,7) - x).days)"
      ],
      "metadata": {
        "id": "P8VaTtjC0jx-"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2['siretEtablissementSuccesseur'] = siret_s2['siretEtablissementSuccesseur'].fillna(0)"
      ],
      "metadata": {
        "id": "AxIHR8LD0jx-"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2['has_succ'] = siret_s2['siretEtablissementSuccesseur'].apply(lambda x: 0 if int(x)==0 else 1)"
      ],
      "metadata": {
        "id": "piD2epbz0jx_"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2 = siret_s2.add_prefix('s2_').rename(columns={'s2_SIRET':'SIRET'})"
      ],
      "metadata": {
        "id": "wbKLbBWETaEA"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siret_s2['s2_siretEtablissementSuccesseur'] = siret_s2['s2_siretEtablissementSuccesseur'].astype(str)"
      ],
      "metadata": {
        "id": "DHWc0kllZBh0"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I am left with two dataframes, which I can merge and where SIRET numbers have a predecessor or successor it will show and give my models more complex relationships to learn."
      ],
      "metadata": {
        "id": "t-KySKlbf8LV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Combination**"
      ],
      "metadata": {
        "id": "i1ewXUuerIcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To merge on SIRET number I have to ensure all entries match, so I will convert all numbers to strings and then strip them of blanks.  "
      ],
      "metadata": {
        "id": "FBw7Q7QbhomZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "xlZPZQURFWXO"
      },
      "outputs": [],
      "source": [
        "data['SIRET'] = data['SIRET'].astype('str')\n",
        "data['SIRET'] = data['SIRET'].str.strip()\n",
        "siret_n['SIRET'] = siret_n['SIRET'].astype(str)\n",
        "siret_n['SIRET'] = siret_n['SIRET'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "kEgnl5VzoTJ9"
      },
      "outputs": [],
      "source": [
        "data['SIRET'] = data['SIRET'].str.lstrip('0')\n",
        "data_final = data.merge(siret_n, left_on='SIRET', right_on='SIRET', how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the below are unnessecary columns that are either not useful or categorical data that needs to be dropped."
      ],
      "metadata": {
        "id": "FWztrFBrkT02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "UV7PCU7WwcjH"
      },
      "outputs": [],
      "source": [
        "data_cols = data.drop(columns=(['department', 'pop_male_departement', 'death_departement_deces_x', 'death_departement_deces_y', 'DEP', 'region', 'concat_2',\n",
        "                                'concat_3', 'Code_postal', 'APP_Libelle_etablissement', 'Adresse_2_UA', 'Numero_inspection', 'geores', 'Synthese_eval_sanit', 'Agrement',\n",
        "                                'APP_Libelle_activite_etablissement', 'ods_type_activite', 'filtre', 'Libelle_commune', 'Date_inspection', 'dayname', 'season', 'trustii_id',\n",
        "                                ])).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I can start merging my SIRET Stock Files."
      ],
      "metadata": {
        "id": "7QB_8jGmk3ND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "Okfir2gmyMgi"
      },
      "outputs": [],
      "source": [
        "data['SIRET'] = data['SIRET'].astype('str')\n",
        "data['SIRET'] = data['SIRET'].str.strip()\n",
        "siret_n['SIRET'] = siret_n['SIRET'].astype(str)\n",
        "siret_n['SIRET'] = siret_n['SIRET'].str.strip()\n",
        "data['SIRET'] = data['SIRET'].str.lstrip('0')\n",
        "data_final = data[data_cols].merge(siret_n, left_on='SIRET', right_on='SIRET', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "7NzZq3sBNcQV"
      },
      "outputs": [],
      "source": [
        "data_final = data_final.merge(siret_l, left_on='siren', right_on='siren', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "x4YdSs1D-Ru1"
      },
      "outputs": [],
      "source": [
        "siret_s['SIRET'] = siret_s['SIRET'].astype(str)\n",
        "siret_s['SIRET'] = siret_s['SIRET'].str.strip()\n",
        "data_final = data_final.merge(siret_s, left_on='SIRET', right_on='SIRET', how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best pre-processing method I found was Min Max Scaling."
      ],
      "metadata": {
        "id": "SDgMqop3gusQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "izQS0Ivb3_3x"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.MinMaxScaler()\n",
        "d = scaler.fit_transform(data_final.drop(columns=['SIRET']))\n",
        "scaled_df = pd.DataFrame(d, columns=data_final.drop(columns=['SIRET']).columns)\n",
        "scaled_df = pd.concat([scaled_df, data['Synthese_eval_sanit'], data['trustii_id']], axis=1, join=\"inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I experimented with other methods for dealing with blanks however the most successful by far was `IterativeImputer()`. Due to computational needs I cannot run my whole dataset through it. So I will run my data without the additional SIRET datasets then merge back together after blank values have been dealt with."
      ],
      "metadata": {
        "id": "OOlAJdt8gx_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iyppiUswTSU",
        "outputId": "b9470253-838d-490d-b63d-314e9c0fe5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "imputer = IterativeImputer()\n",
        "X_imputed = imputer.fit_transform(scaled_df[np.delete(data_cols,0)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "tUoHKvZywqtW"
      },
      "outputs": [],
      "source": [
        "imputed_df = pd.DataFrame(X_imputed, columns=scaled_df[np.delete(data_cols,0)].columns)\n",
        "imputed_df = pd.concat([scaled_df.drop(columns=imputed_df.columns), imputed_df], axis=1, join=\"inner\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "asAT9_kpqrMZ",
        "outputId": "83de5a44-7dbb-4584-809a-0b2c990b477b"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          siren       nic  n_statutDiffusionEtablissement  \\\n",
              "0      0.509310  0.000371                             0.0   \n",
              "1      0.906221  0.000142                             0.0   \n",
              "2      0.488732  0.000142                             0.0   \n",
              "3      0.806150  0.000240                             0.0   \n",
              "4      0.535330  0.000207                             0.0   \n",
              "...         ...       ...                             ...   \n",
              "32383  0.452068  0.009454                             0.0   \n",
              "32384  0.323531  0.000251                             0.0   \n",
              "32385  0.410614  0.000186                             0.0   \n",
              "32386  0.949483  0.000229                             0.0   \n",
              "32387  0.909975  0.000131                             0.0   \n",
              "\n",
              "       n_dateCreationEtablissement  n_dateDernierTraitementEtablissement  \\\n",
              "0                         0.096452                              0.014433   \n",
              "1                         0.016987                              0.086281   \n",
              "2                         0.132589                              0.014433   \n",
              "3                         0.062359                              0.014433   \n",
              "4                         0.091044                              0.723077   \n",
              "...                            ...                                   ...   \n",
              "32383                     0.131808                              0.014433   \n",
              "32384                     0.306224                              0.014433   \n",
              "32385                     0.199495                              0.001586   \n",
              "32386                     0.002684                              0.017764   \n",
              "32387                     0.015324                              0.110230   \n",
              "\n",
              "       n_etablissementSiege  n_nombrePeriodesEtablissement  \\\n",
              "0                       1.0                         0.1250   \n",
              "1                       1.0                         0.1250   \n",
              "2                       1.0                         0.1875   \n",
              "3                       1.0                         0.0625   \n",
              "4                       1.0                         0.0625   \n",
              "...                     ...                            ...   \n",
              "32383                   0.0                         0.1250   \n",
              "32384                   1.0                         0.2500   \n",
              "32385                   1.0                         0.2500   \n",
              "32386                   1.0                         0.0625   \n",
              "32387                   1.0                         0.0625   \n",
              "\n",
              "       n_libelleCommuneEtrangerEtablissement  n_codeCommuneEtablissement  \\\n",
              "0                                        0.0                    0.335037   \n",
              "1                                        0.0                    0.943861   \n",
              "2                                        0.0                    0.000000   \n",
              "3                                        0.0                    0.512732   \n",
              "4                                        0.0                    0.000000   \n",
              "...                                      ...                         ...   \n",
              "32383                                    0.0                    0.953607   \n",
              "32384                                    0.0                    0.761705   \n",
              "32385                                    0.0                    0.367284   \n",
              "32386                                    0.0                    0.428118   \n",
              "32387                                    0.0                    0.669364   \n",
              "\n",
              "       n_codePaysEtrangerEtablissement  ...  pop_male_departement_Loiret  \\\n",
              "0                                  0.0  ...                          0.0   \n",
              "1                                  0.0  ...                          0.0   \n",
              "2                                  0.0  ...                          0.0   \n",
              "3                                  0.0  ...                          0.0   \n",
              "4                                  0.0  ...                          0.0   \n",
              "...                                ...  ...                          ...   \n",
              "32383                              0.0  ...                          0.0   \n",
              "32384                              0.0  ...                          0.0   \n",
              "32385                              0.0  ...                          0.0   \n",
              "32386                              0.0  ...                          0.0   \n",
              "32387                              0.0  ...                          0.0   \n",
              "\n",
              "       pop_male_departement_Tarn-et-Garonne  \\\n",
              "0                                       0.0   \n",
              "1                                       0.0   \n",
              "2                                       0.0   \n",
              "3                                       0.0   \n",
              "4                                       0.0   \n",
              "...                                     ...   \n",
              "32383                                   0.0   \n",
              "32384                                   0.0   \n",
              "32385                                   0.0   \n",
              "32386                                   0.0   \n",
              "32387                                   0.0   \n",
              "\n",
              "       pop_male_departement_Eure-et-Loir  pop_male_departement_Lozère  \\\n",
              "0                                    0.0                          0.0   \n",
              "1                                    0.0                          0.0   \n",
              "2                                    0.0                          0.0   \n",
              "3                                    0.0                          0.0   \n",
              "4                                    0.0                          0.0   \n",
              "...                                  ...                          ...   \n",
              "32383                                0.0                          0.0   \n",
              "32384                                0.0                          0.0   \n",
              "32385                                0.0                          0.0   \n",
              "32386                                0.0                          0.0   \n",
              "32387                                0.0                          0.0   \n",
              "\n",
              "       pop_male_departement_Essonne  pop_male_departement_Haute-Vienne  \\\n",
              "0                               0.0                                0.0   \n",
              "1                               0.0                                0.0   \n",
              "2                               0.0                                0.0   \n",
              "3                               0.0                                0.0   \n",
              "4                               0.0                                0.0   \n",
              "...                             ...                                ...   \n",
              "32383                           0.0                                0.0   \n",
              "32384                           0.0                                0.0   \n",
              "32385                           0.0                                0.0   \n",
              "32386                           0.0                                0.0   \n",
              "32387                           0.0                                0.0   \n",
              "\n",
              "       pop_male_departement_La Réunion  region_avg   dep_avg  post_avg  \n",
              "0                                  0.0    0.698026  0.372693  0.333333  \n",
              "1                                  0.0    0.573311  0.335061  0.590909  \n",
              "2                                  0.0    1.000000  0.774176  0.783333  \n",
              "3                                  0.0    0.682954  0.655078  0.875000  \n",
              "4                                  0.0    1.000000  1.000000  0.666667  \n",
              "...                                ...         ...       ...       ...  \n",
              "32383                              0.0    0.573311  0.570519  0.250000  \n",
              "32384                              0.0    0.573311  0.566873  0.725434  \n",
              "32385                              0.0    0.525567  0.452371  0.644360  \n",
              "32386                              0.0    0.701739  0.596367  0.664062  \n",
              "32387                              0.0    0.535281  0.391238  0.400000  \n",
              "\n",
              "[32388 rows x 1775 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3be1b6eb-c258-4a79-ac5b-40a45e814778\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>siren</th>\n",
              "      <th>nic</th>\n",
              "      <th>n_statutDiffusionEtablissement</th>\n",
              "      <th>n_dateCreationEtablissement</th>\n",
              "      <th>n_dateDernierTraitementEtablissement</th>\n",
              "      <th>n_etablissementSiege</th>\n",
              "      <th>n_nombrePeriodesEtablissement</th>\n",
              "      <th>n_libelleCommuneEtrangerEtablissement</th>\n",
              "      <th>n_codeCommuneEtablissement</th>\n",
              "      <th>n_codePaysEtrangerEtablissement</th>\n",
              "      <th>...</th>\n",
              "      <th>pop_male_departement_Loiret</th>\n",
              "      <th>pop_male_departement_Tarn-et-Garonne</th>\n",
              "      <th>pop_male_departement_Eure-et-Loir</th>\n",
              "      <th>pop_male_departement_Lozère</th>\n",
              "      <th>pop_male_departement_Essonne</th>\n",
              "      <th>pop_male_departement_Haute-Vienne</th>\n",
              "      <th>pop_male_departement_La Réunion</th>\n",
              "      <th>region_avg</th>\n",
              "      <th>dep_avg</th>\n",
              "      <th>post_avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.509310</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.096452</td>\n",
              "      <td>0.014433</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.1250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.335037</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.698026</td>\n",
              "      <td>0.372693</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.906221</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.016987</td>\n",
              "      <td>0.086281</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.1250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.943861</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573311</td>\n",
              "      <td>0.335061</td>\n",
              "      <td>0.590909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.488732</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.132589</td>\n",
              "      <td>0.014433</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.774176</td>\n",
              "      <td>0.783333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.806150</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.062359</td>\n",
              "      <td>0.014433</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.512732</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.682954</td>\n",
              "      <td>0.655078</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.535330</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.091044</td>\n",
              "      <td>0.723077</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32383</th>\n",
              "      <td>0.452068</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.131808</td>\n",
              "      <td>0.014433</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.953607</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573311</td>\n",
              "      <td>0.570519</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32384</th>\n",
              "      <td>0.323531</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.306224</td>\n",
              "      <td>0.014433</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.761705</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573311</td>\n",
              "      <td>0.566873</td>\n",
              "      <td>0.725434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32385</th>\n",
              "      <td>0.410614</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.199495</td>\n",
              "      <td>0.001586</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.367284</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.525567</td>\n",
              "      <td>0.452371</td>\n",
              "      <td>0.644360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32386</th>\n",
              "      <td>0.949483</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002684</td>\n",
              "      <td>0.017764</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.428118</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.701739</td>\n",
              "      <td>0.596367</td>\n",
              "      <td>0.664062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32387</th>\n",
              "      <td>0.909975</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015324</td>\n",
              "      <td>0.110230</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.669364</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.535281</td>\n",
              "      <td>0.391238</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32388 rows × 1775 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3be1b6eb-c258-4a79-ac5b-40a45e814778')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3be1b6eb-c258-4a79-ac5b-40a45e814778 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3be1b6eb-c258-4a79-ac5b-40a45e814778');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-81d7f0c3-0bc7-4479-a265-82950ee4495f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-81d7f0c3-0bc7-4479-a265-82950ee4495f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-81d7f0c3-0bc7-4479-a265-82950ee4495f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "imputed_df"
            }
          },
          "metadata": {},
          "execution_count": 313
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}